{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd2fc49",
   "metadata": {},
   "source": [
    "# Week 2: Label Flipping Attack - Fetal Plane Classification (Google Colab)\n",
    "\n",
    "This notebook demonstrates a **label flipping poisoning attack** in federated learning on fetal ultrasound plane classification.\n",
    "\n",
    "## üìã Before Running:\n",
    "1. Upload your code folder (`week2_attack/`) to Google Drive\n",
    "2. Upload your dataset folder (`FETAL/`) to Google Drive\n",
    "3. Update the paths in Section 1 to match your Drive structure\n",
    "4. **Recommended**: Run `colab_week1_baseline.ipynb` first for comparison\n",
    "\n",
    "## Attack Scenario\n",
    "- **10 hospitals/clinics** (clients) collaborate\n",
    "- **30% are malicious** (3 out of 10 clients)\n",
    "- **Attack**: Malicious clients flip labels to poison the global model\n",
    "- **Goal**: Show how attacks degrade model performance compared to baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724de225",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive and Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ‚ö†Ô∏è CHANGE THESE PATHS TO MATCH YOUR GOOGLE DRIVE STRUCTURE\n",
    "DRIVE_BASE = '/content/drive/MyDrive/fetal_plane_implementation'\n",
    "CODE_DIR = f'{DRIVE_BASE}/week2_attack'\n",
    "DATA_DIR = f'{DRIVE_BASE}/FETAL'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add code directory to Python path (so we can import modules)\n",
    "sys.path.insert(0, CODE_DIR)\n",
    "\n",
    "# DON'T change directory - stay in /content\n",
    "# Just add the path so Python can find the modules\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Google Drive Mounted Successfully\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìÇ Code directory: {CODE_DIR}\")\n",
    "print(f\"üìÇ Data directory: {DATA_DIR}\")\n",
    "print(f\"üìÇ Current working directory: {os.getcwd()}\")\n",
    "print(f\"üìÇ Python can import from: {CODE_DIR in sys.path}\")\n",
    "print(\"\\nüìÅ Files in code directory:\")\n",
    "try:\n",
    "    print([f for f in os.listdir(CODE_DIR) if f.endswith('.py')])\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è  Directory not found: {CODE_DIR}\")\n",
    "    print(\"Please check your DRIVE_BASE path above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca915cea",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision pandas pillow numpy matplotlib -q\n",
    "\n",
    "print(\"‚úÖ Dependencies installed/verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047dc12",
   "metadata": {},
   "source": [
    "## 3. Update Config for Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import config and override DATA_DIR\n",
    "from config import Config\n",
    "\n",
    "# Override data directory to point to Google Drive\n",
    "Config.DATA_DIR = DATA_DIR\n",
    "\n",
    "print(f\"‚úÖ Config updated: DATA_DIR = {Config.DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847774ea",
   "metadata": {},
   "source": [
    "## 4. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "# Import local modules from Drive\n",
    "from data_loader import load_fetal_plane_data, split_non_iid_dirichlet, get_client_loaders\n",
    "from model import get_model\n",
    "from server import Server\n",
    "from client import Client\n",
    "from attack import LabelFlipAttacker\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ All modules imported successfully\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"Running on CPU (training will be slower)\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199cd98",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Federated Learning - FETAL PLANE CLASSIFICATION\")\n",
    "print(\"NON-IID WITH LABEL FLIPPING ATTACK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Clients: {Config.NUM_CLIENTS} (simulating hospitals/clinics)\")\n",
    "print(f\"Malicious Clients: {Config.NUM_MALICIOUS} ({Config.NUM_MALICIOUS/Config.NUM_CLIENTS*100:.0f}%)\")\n",
    "print(f\"Attack Type: Label Flipping\")\n",
    "print(f\"Rounds: {Config.NUM_ROUNDS}\")\n",
    "print(f\"Local epochs: {Config.LOCAL_EPOCHS}\")\n",
    "print(f\"Data Distribution: NON-IID (Dirichlet Œ±={Config.DIRICHLET_ALPHA})\")\n",
    "print(f\"Model: {Config.MODEL_TYPE}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö†Ô∏è  WARNING: 30% of clients will flip labels to poison the model!\")\n",
    "print(\"Expected: Model accuracy will degrade compared to baseline\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188bc9f3",
   "metadata": {},
   "source": [
    "## 6. Load Fetal Plane Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a530159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading fetal plane data from Google Drive...\\n\")\n",
    "train_dataset, test_dataset = load_fetal_plane_data()\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Total test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Show class distribution\n",
    "train_labels = [train_dataset.targets[i] for i in range(len(train_dataset))]\n",
    "class_counts = Counter(train_labels)\n",
    "class_names = ['Fetal abdomen', 'Fetal brain', 'Fetal femur', 'Fetal thorax', 'Maternal cervix', 'Other']\n",
    "print(\"\\nClass distribution in training data:\")\n",
    "for cls, count in sorted(class_counts.items()):\n",
    "    print(f\"  Class {cls} ({class_names[cls]}): {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e7881",
   "metadata": {},
   "source": [
    "## 7. Create Non-IID Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7869eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating Non-IID data split with Dirichlet(Œ±={})...\\n\".format(Config.DIRICHLET_ALPHA))\n",
    "\n",
    "client_data_indices = split_non_iid_dirichlet(\n",
    "    train_dataset,\n",
    "    num_clients=Config.NUM_CLIENTS,\n",
    "    alpha=Config.DIRICHLET_ALPHA,\n",
    "    num_classes=Config.NUM_CLASSES\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Non-IID split created!\")\n",
    "print(\"\\nData distribution per client:\")\n",
    "for client_id, indices in enumerate(client_data_indices):\n",
    "    labels = [train_dataset.targets[i] for i in indices]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    dominant_class = unique_labels[np.argmax(counts)]\n",
    "    dominant_count = counts[np.argmax(counts)]\n",
    "    client_type = \"üî¥ MALICIOUS\" if client_id < Config.NUM_MALICIOUS else \"‚úÖ HONEST\"\n",
    "    print(f\"  Client {client_id} [{client_type}]: {len(indices):4d} samples, dominant={dominant_class} ({class_names[dominant_class]}, {dominant_count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8101d2",
   "metadata": {},
   "source": [
    "## 8. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f724ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_loaders = get_client_loaders(\n",
    "    train_dataset,\n",
    "    client_data_indices,\n",
    "    batch_size=Config.BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(client_loaders)} client data loaders\")\n",
    "print(f\"‚úÖ Test loader has {len(test_loader.dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c717d88",
   "metadata": {},
   "source": [
    "## 9. Initialize Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6172ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInitializing global model...\")\n",
    "global_model = get_model(num_classes=Config.NUM_CLASSES, pretrained=True)\n",
    "global_model = global_model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in global_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in global_model.parameters() if p.requires_grad)\n",
    "print(f\"‚úÖ Model initialized on {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df735a2",
   "metadata": {},
   "source": [
    "## 10. Create Server and Clients (with Attackers)\n",
    "\n",
    "**Key**: First 3 clients will be malicious attackers that flip labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize server\n",
    "server = Server(global_model, test_loader)\n",
    "print(\"‚úÖ Server initialized\\n\")\n",
    "\n",
    "# Create clients with attackers\n",
    "print(\"üî¥ Creating clients (including malicious attackers)...\\n\")\n",
    "clients = []\n",
    "attackers = []\n",
    "\n",
    "for i in range(Config.NUM_CLIENTS):\n",
    "    if i < Config.NUM_MALICIOUS:\n",
    "        # Malicious client with label flip attack\n",
    "        attacker = LabelFlipAttacker(\n",
    "            client_id=i,\n",
    "            train_loader=client_loaders[i],\n",
    "            learning_rate=Config.LEARNING_RATE,\n",
    "            local_epochs=Config.LOCAL_EPOCHS,\n",
    "            num_classes=Config.NUM_CLASSES\n",
    "        )\n",
    "        clients.append(attacker)\n",
    "        attackers.append(attacker)\n",
    "        print(f\"  üî¥ Client {i}: MALICIOUS (Label Flipping)\")\n",
    "    else:\n",
    "        # Honest client\n",
    "        client = Client(\n",
    "            client_id=i,\n",
    "            train_loader=client_loaders[i],\n",
    "            learning_rate=Config.LEARNING_RATE,\n",
    "            local_epochs=Config.LOCAL_EPOCHS\n",
    "        )\n",
    "        clients.append(client)\n",
    "        print(f\"  ‚úÖ Client {i}: HONEST\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total: {len(clients)} clients ({len(attackers)} malicious, {len(clients)-len(attackers)} honest)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0b15a",
   "metadata": {},
   "source": [
    "## 11. Evaluate Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321acf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating initial model...\")\n",
    "initial_acc = server.evaluate()\n",
    "print(f\"\\nüìä Initial Test Accuracy: {initial_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88741c",
   "metadata": {},
   "source": [
    "## 12. Federated Training Loop (Under Attack)\n",
    "\n",
    "‚ö†Ô∏è **Attack in Action**: Malicious clients will flip labels during training!\n",
    "\n",
    "Watch how the accuracy degrades compared to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47535923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "round_accuracies = [initial_acc]\n",
    "round_losses = []\n",
    "attack_norms = []  # Track attack update norms\n",
    "honest_norms = []  # Track honest update norms\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING FEDERATED TRAINING (WITH ATTACK)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for round_num in range(1, Config.NUM_ROUNDS + 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ROUND {round_num}/{Config.NUM_ROUNDS}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Client training phase\n",
    "    print(\"\\n[CLIENT TRAINING]\")\n",
    "    client_updates = []\n",
    "    client_weights = []\n",
    "    round_attack_norms = []\n",
    "    round_honest_norms = []\n",
    "    round_train_losses = []\n",
    "    \n",
    "    for client in clients:\n",
    "        update, train_acc, train_loss, update_norm = client.train(global_model)\n",
    "        client_updates.append(update)\n",
    "        client_weights.append(len(client.train_loader.dataset))\n",
    "        round_train_losses.append(train_loss)\n",
    "        \n",
    "        is_malicious = client.client_id < Config.NUM_MALICIOUS\n",
    "        client_type = \"üî¥ MALICIOUS\" if is_malicious else \"‚úÖ HONEST\"\n",
    "        \n",
    "        if is_malicious:\n",
    "            round_attack_norms.append(update_norm)\n",
    "        else:\n",
    "            round_honest_norms.append(update_norm)\n",
    "        \n",
    "        print(f\"  Client {client.client_id} [{client_type}]: Loss={train_loss:.4f}, Acc={train_acc:.2f}%, Norm={update_norm:.4f}\")\n",
    "    \n",
    "    attack_norms.append(np.mean(round_attack_norms))\n",
    "    honest_norms.append(np.mean(round_honest_norms))\n",
    "    \n",
    "    avg_loss = np.mean(round_train_losses)\n",
    "    round_losses.append(avg_loss)\n",
    "    \n",
    "    # Server aggregation (no defense - accepts all updates)\n",
    "    print(\"\\n[SERVER AGGREGATION]\")\n",
    "    global_model = server.aggregate_updates(client_updates, client_weights)\n",
    "    print(\"‚ö†Ô∏è  Server aggregated ALL updates (including malicious ones!)\")\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\n[EVALUATION]\")\n",
    "    test_acc = server.evaluate()\n",
    "    round_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f\"\\nüìä Round {round_num} Results:\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"   Change: {test_acc - round_accuracies[-2]:+.2f}%\")\n",
    "    print(f\"   Best so far: {max(round_accuracies):.2f}%\")\n",
    "    print(f\"   Avg Malicious Norm: {round_attack_norms[-1] if round_attack_norms else 0:.4f}\")\n",
    "    print(f\"   Avg Honest Norm: {round_honest_norms[-1] if round_honest_norms else 0:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b1cb1",
   "metadata": {},
   "source": [
    "## 13. Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5cc4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETED (UNDER ATTACK)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nInitial Accuracy: {initial_acc:.2f}%\")\n",
    "print(f\"Final Accuracy: {round_accuracies[-1]:.2f}%\")\n",
    "print(f\"Change: {round_accuracies[-1] - initial_acc:+.2f}%\")\n",
    "print(f\"Best Accuracy: {max(round_accuracies):.2f}%\")\n",
    "print(f\"Worst Accuracy: {min(round_accuracies):.2f}%\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  ATTACK IMPACT:\")\n",
    "print(f\"   {Config.NUM_MALICIOUS} out of {Config.NUM_CLIENTS} clients were malicious\")\n",
    "print(f\"   Label flipping poisoned the training process\")\n",
    "print(f\"   Expected: Lower accuracy than baseline (honest clients only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d9d90",
   "metadata": {},
   "source": [
    "## 14. Visualize Attack Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dccd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Accuracy over rounds\n",
    "axes[0, 0].plot(range(len(round_accuracies)), round_accuracies, 'r-o', linewidth=2, markersize=8, label='With Attack')\n",
    "axes[0, 0].set_xlabel('Round', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "axes[0, 0].set_title('Accuracy Under Label Flipping Attack', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Training loss\n",
    "axes[0, 1].plot(range(1, len(round_losses) + 1), round_losses, 'orange', linewidth=2, markersize=8, marker='o')\n",
    "axes[0, 1].set_xlabel('Round', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Average Training Loss', fontsize=12)\n",
    "axes[0, 1].set_title('Training Loss (Poisoned)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Update norms comparison\n",
    "axes[1, 0].plot(range(1, len(attack_norms) + 1), attack_norms, 'r-o', linewidth=2, markersize=6, label='Malicious')\n",
    "axes[1, 0].plot(range(1, len(honest_norms) + 1), honest_norms, 'g-s', linewidth=2, markersize=6, label='Honest')\n",
    "axes[1, 0].set_xlabel('Round', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Average Update Norm', fontsize=12)\n",
    "axes[1, 0].set_title('Malicious vs Honest Update Norms', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Accuracy degradation\n",
    "accuracy_changes = [round_accuracies[i+1] - round_accuracies[i] for i in range(len(round_accuracies)-1)]\n",
    "colors = ['green' if x > 0 else 'red' for x in accuracy_changes]\n",
    "axes[1, 1].bar(range(1, len(accuracy_changes) + 1), accuracy_changes, color=colors, alpha=0.7)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1, 1].set_xlabel('Round', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Accuracy Change (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Round-to-Round Accuracy Change', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DRIVE_BASE}/week2_attack_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Plot saved to: {DRIVE_BASE}/week2_attack_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129adb92",
   "metadata": {},
   "source": [
    "## 15. Compare with Baseline (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8bdf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results if available\n",
    "import pickle\n",
    "baseline_file = f'{DRIVE_BASE}/week1_baseline_results.pkl'\n",
    "\n",
    "if os.path.exists(baseline_file):\n",
    "    with open(baseline_file, 'rb') as f:\n",
    "        baseline_results = pickle.load(f)\n",
    "    \n",
    "    baseline_accs = baseline_results['accuracies']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON: BASELINE vs ATTACK\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBaseline (Honest) Final Accuracy: {baseline_accs[-1]:.2f}%\")\n",
    "    print(f\"Attack (30% Malicious) Final Accuracy: {round_accuracies[-1]:.2f}%\")\n",
    "    print(f\"Performance Degradation: {baseline_accs[-1] - round_accuracies[-1]:.2f}%\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(baseline_accs)), baseline_accs, 'b-o', linewidth=2, markersize=8, label='Baseline (Honest)')\n",
    "    plt.plot(range(len(round_accuracies)), round_accuracies, 'r-o', linewidth=2, markersize=8, label='With Attack (30% Malicious)')\n",
    "    plt.xlabel('Round', fontsize=12)\n",
    "    plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    plt.title('Baseline vs Attack: Impact on Model Performance', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f'{DRIVE_BASE}/baseline_vs_attack_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\n‚úÖ Comparison plot saved to: {DRIVE_BASE}/baseline_vs_attack_comparison.png\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Baseline results not found at {baseline_file}\")\n",
    "    print(\"Run colab_week1_baseline.ipynb first to compare results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d1c03",
   "metadata": {},
   "source": [
    "## 16. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e08c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save poisoned model to Google Drive\n",
    "model_path = f'{DRIVE_BASE}/fetal_plane_poisoned_model.pth'\n",
    "torch.save(global_model.state_dict(), model_path)\n",
    "print(f\"‚úÖ Poisoned model saved to: {model_path}\")\n",
    "\n",
    "# Save results to Google Drive\n",
    "results = {\n",
    "    'accuracies': round_accuracies,\n",
    "    'losses': round_losses,\n",
    "    'attack_norms': attack_norms,\n",
    "    'honest_norms': honest_norms,\n",
    "    'config': {\n",
    "        'num_clients': Config.NUM_CLIENTS,\n",
    "        'num_malicious': Config.NUM_MALICIOUS,\n",
    "        'num_rounds': Config.NUM_ROUNDS,\n",
    "        'local_epochs': Config.LOCAL_EPOCHS,\n",
    "        'alpha': Config.DIRICHLET_ALPHA\n",
    "    }\n",
    "}\n",
    "\n",
    "import pickle\n",
    "results_path = f'{DRIVE_BASE}/week2_attack_results.pkl'\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"‚úÖ Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87579b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Attack Details:\n",
    "\n",
    "1. **Attack Type**: Label Flipping\n",
    "   - Malicious clients randomly flip labels during training\n",
    "   - Poisons the gradient updates sent to server\n",
    "\n",
    "2. **Attack Scale**: 30% malicious clients (3 out of 10)\n",
    "\n",
    "3. **Server Defense**: None (accepts all updates)\n",
    "\n",
    "### Observed Impact:\n",
    "\n",
    "- **Accuracy Degradation**: Model performance significantly lower than baseline\n",
    "- **Unstable Training**: Accuracy may fluctuate or fail to improve\n",
    "- **Update Norms**: Malicious updates may have different magnitudes\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. Even a minority (30%) of malicious clients can severely degrade model performance\n",
    "2. Simple averaging (FedAvg) without defense is vulnerable to poisoning\n",
    "3. The attack is stealthy - server cannot distinguish malicious updates\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Week 6**: Apply full defense mechanisms (run `colab_week6_full_defense.ipynb`)\n",
    "\n",
    "### Files Saved to Google Drive:\n",
    "\n",
    "- Model: `fetal_plane_poisoned_model.pth`\n",
    "- Results: `week2_attack_results.pkl`\n",
    "- Plots: `week2_attack_results.png`, `baseline_vs_attack_comparison.png`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
