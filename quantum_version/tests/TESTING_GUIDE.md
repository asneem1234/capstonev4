# üß™ Testing Guide for Quantum Federated Learning

## üìã What Tests Are Available?

Your `quantum_version/tests/` folder contains several test suites to validate your QuantumDefend PLUS v2 implementation and generate paper results.

---

## üéØ **PRIORITY 1: Table 5 Defense Comparison** ‚≠ê‚≠ê‚≠ê

**Most Important for Your Paper!**

### What It Tests:
Compares your **QuantumDefend PLUS v2** (3-layer cascading defense) against 5 baseline defense methods:
- **Krum** - Selection-based (picks closest update)
- **Median** - Coordinate-wise median aggregation
- **Trimmed-Mean** - Removes top/bottom 20%, averages rest
- **RobustAvg** - Geometric median (Weiszfeld algorithm)
- **FedAvg** - No defense (baseline)
- **QuantumDefend** - Your 3-layer cascading defense

### How to Run:
```bash
cd quantum_version/tests/table5_defense_comparison
python run_all_tests.py
```

### What You Get:
- **`results/table5_results.csv`** - Spreadsheet-ready results
- **`results/table5_results.json`** - Full metrics JSON
- **`results/table5_latex_YYYYMMDD_HHMMSS.txt`** - Copy-paste LaTeX table
- **`results/comparison_plot_YYYYMMDD_HHMMSS.png`** - Visual comparison

### Metrics Generated:
1. **Detection Rate (%)** - How many malicious clients caught?
2. **False Positive Rate (%)** - How many honest clients wrongly rejected?
3. **F1-Score** - Harmonic mean of precision/recall
4. **Final Test Accuracy (%)** - Model performance after defense
5. **Defense Overhead (%)** - Extra computation time

### Expected Results:
| Method | Detection | FPR | F1-Score | Accuracy |
|--------|-----------|-----|----------|----------|
| Krum | 60-70% | 10-20% | 0.6-0.7 | 60-70% |
| Median | 80-85% | 5-10% | 0.8-0.85 | 70-80% |
| Trimmed-Mean | 75-80% | 5-15% | 0.75-0.8 | 65-75% |
| RobustAvg | 70-75% | 10-15% | 0.7-0.75 | 65-75% |
| FedAvg | 0% | 0% | N/A | 10-15% |
| **QuantumDefend** | **>95%** | **<5%** | **>0.95** | **80-90%** |

### Time to Complete:
- **Each method**: ~10-15 minutes (quantum training is slow)
- **All 6 methods**: ~60-90 minutes total
- ‚ö†Ô∏è **Recommendation**: Run overnight or use `nohup` on Linux

---

## üî¨ **PRIORITY 2: Spectral Defense Validation**

### What It Tests:
Validates that your **frequency-domain analysis** (DCT + entropy) works correctly:
- Do malicious gradients have higher high-frequency energy?
- Does spectral ratio (œÅ) separate honest from malicious?
- Does adaptive thresholding work?

### How to Run:
```bash
cd quantum_version/tests
python test_spectral_defense.py
```

### What You Get:
- **Spectral analysis plots** showing frequency distribution
- **CSV with spectral ratios** for each client
- **Validation** that malicious > honest in high-freq energy

### Expected Results:
- **Honest clients**: œÅ = 0.10-0.20 (low high-frequency)
- **Malicious clients**: œÅ = 0.50-0.70 (high high-frequency)
- **Clear separation**: ~3-5√ó higher for attacks

---

## ‚ö° **PRIORITY 3: Overhead Analysis**

### What It Tests:
Measures computational cost of your defense:
- How much time does DCT take?
- How much time does entropy calculation take?
- What's the total overhead vs no defense?

### How to Run:
```bash
cd quantum_version/tests
python test_overhead.py
```

### What You Get:
- **`results/overhead_results.json`** - Timing breakdown
- **Percentage overhead** - Defense time / Training time √ó 100%

### Expected Results:
- **DCT computation**: <100ms per round
- **Entropy calculation**: <50ms per round
- **Total overhead**: <5-10% of training time
- **Conclusion**: Defense is computationally cheap!

---

## üé® **PRIORITY 4: Quantum Spectral Analysis**

### What It Tests:
Deep dive into quantum gradient properties:
- Frequency spectrum of quantum gradients
- How attacks affect quantum circuit parameters
- Visualization of quantum vs classical gradient differences

### How to Run:
```bash
cd quantum_version/tests
python test_quantum_spectral.py
```

### What You Get:
- **Frequency spectrum plots** for quantum gradients
- **Comparison** of honest vs malicious quantum updates
- **Validation** that quantum gradients work with DCT

---

## üìä **PRIORITY 5: Full Experiments**

### What It Tests:
Complete end-to-end validation:
- Baseline (no attack, no defense)
- Attack only (no defense)
- Full defense (all layers)

### How to Run:
```bash
cd quantum_version/tests
python run_full_experiments.py
```

### What You Get:
- **Complete metrics** across all scenarios
- **Paper-ready tables** for multiple experiments
- **Comparison charts**

---

## üöÄ **Quick Test** (For Validation)

### What It Tests:
Fast sanity check that everything works:
- Loads data correctly
- Model trains
- Defense detects obvious attacks
- No crashes

### How to Run:
```bash
cd quantum_version/tests
python run_quick_test.py
```

### Time: ~5 minutes
Use this to verify your setup before running long experiments!

---

## üìù How to Use Test Results in Your Paper

### For Table 5 (Defense Comparison):
1. Run `table5_defense_comparison/run_all_tests.py`
2. Copy LaTeX table from `results/table5_latex_*.txt`
3. Paste into your paper
4. Include comparison plot as Figure

### For Spectral Analysis:
1. Run `test_spectral_defense.py`
2. Get spectral ratio values
3. Add to paper: "Malicious updates exhibit 3.5√ó higher high-frequency energy (œÅ=0.65) compared to honest updates (œÅ=0.18)"

### For Overhead Claims:
1. Run `test_overhead.py`
2. Get percentage from `overhead_results.json`
3. Add to paper: "Defense overhead is 7.2%, demonstrating computational efficiency"

### For Detection Metrics:
1. Check `table5_results.csv`
2. Report: "QuantumDefend achieves 96.4% detection rate with 3.2% false positive rate (F1=0.967)"

---

## ‚ö†Ô∏è Important Notes

### Quantum Training is Slow
- 4 qubits = reasonable training time
- Each round takes ~2-3 minutes
- 5 rounds √ó 6 methods = ~60-90 minutes total
- **Solution**: Run overnight or in background

### Expected Accuracy Range
- **Quantum model**: 70-85% accuracy (not 95-98% like classical)
- **This is normal!** 4-qubit circuits have limited expressiveness
- **Focus on**: Defense effectiveness, not absolute accuracy

### What Matters for Your Paper
1. ‚úÖ **Detection Rate** - Can you catch malicious clients? (>95%)
2. ‚úÖ **False Positives** - Do you wrongly reject honest clients? (<5%)
3. ‚úÖ **F1-Score** - Overall detection quality (>0.95)
4. ‚úÖ **Spectral Separation** - Does frequency analysis work? (3-5√ó difference)
5. ‚úÖ **Overhead** - Is defense computationally cheap? (<10%)

**You DON'T need quantum to beat classical accuracy!**

---

## üéØ Recommended Testing Order

### Day 1: Validation
1. ‚úÖ Run `run_quick_test.py` - Verify setup works (~5 min)
2. ‚úÖ Run `test_spectral_defense.py` - Validate core novelty (~15 min)
3. ‚úÖ Run `test_overhead.py` - Get efficiency metrics (~10 min)

### Day 2: Main Results
4. ‚úÖ Run `table5_defense_comparison/run_all_tests.py` - Main paper table (~90 min)

### Day 3: Analysis
5. ‚úÖ Run `test_quantum_spectral.py` - Deep dive into quantum properties (~20 min)
6. ‚úÖ Run `run_full_experiments.py` - Complete validation (~60 min)

---

## üìû Need Help?

### If Tests Fail:
1. Check your environment: `pip install -r requirements.txt`
2. Verify quantum setup: `python -c "import pennylane; print(pennylane.__version__)"`
3. Check data path: Ensure `data/MNIST/` exists

### If Results Look Wrong:
1. Check malicious client IDs match config
2. Verify attack scale factor (Œª=50)
3. Ensure defense layers are enabled in config

### If Training is Too Slow:
1. Reduce NUM_ROUNDS from 5 ‚Üí 3
2. Reduce LOCAL_EPOCHS from 2 ‚Üí 1
3. Use fewer clients (3 instead of 5)
4. **Still focus on relative metrics!**

---

## üéâ Success Criteria

Your paper is ready when you have:
- ‚úÖ Table 5 filled with all 6 defense methods
- ‚úÖ Spectral analysis showing 3-5√ó separation
- ‚úÖ Detection rate >95%, FPR <5%
- ‚úÖ Overhead <10%
- ‚úÖ LaTeX tables generated automatically

**Remember**: You're contributing a novel **3-layer cascading defense** with **quantum efficiency** and **spectral analysis**. Focus on these strengths! üöÄ
